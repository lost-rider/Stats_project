{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_dzyhWfBdWK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwfSXwmtp7FA"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data as data\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import cv2\n",
        "import gdown\n",
        "import zipfile\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89LElWlvDZfU"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Path to the folder you want to delete\n",
        "folder_path = '/content/data'\n",
        "folder_path2 = '/content/reslt_3hidden_layers_test_2'\n",
        "\n",
        "# Delete the folder\n",
        "shutil.rmtree(folder_path)\n",
        "shutil.rmtree(folder_path2)\n",
        "\n",
        "print(f\"Folder '{folder_path}' has been deleted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk04MoqUqDCR"
      },
      "source": [
        "#  Device and parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmZaCr4rp7FC"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device(\"cpu\")\n",
        "N = 128  # Batch size\n",
        "log_interval = 150  # Plotting frequency\n",
        "num_epoch = 10  # Number of epochs\n",
        "size_data_train = 4000\n",
        "\n",
        "lr = 1e-3  # Learning rate\n",
        "reg_w_decay = 1e-2  # Weight decay\n",
        "args_dico = {\n",
        "    'name': 'reslt_3hidden_layers_old_test',\n",
        "    'save_dir': './reslt_3hidden_layers_test_2',\n",
        "}\n",
        "\n",
        "args_dico2 = args_dico.copy()\n",
        "seed = 2\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Create results directory\n",
        "os.makedirs(args_dico['save_dir'], exist_ok=True)\n",
        "print(f\"Directory {args_dico['save_dir']} is ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnWC1Q4kp7FD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class DatasetFromFolder(data.Dataset):\n",
        "\n",
        "    def __init__(self, data_feature, data_target,transform=None,phase='train'):\n",
        "        self.data_feature = data_feature\n",
        "        self.data_target = data_target\n",
        "        self.transform =transform\n",
        "        self.phase=phase\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_feature)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load image as ndarray type (Height * Width * Channels)\n",
        "\n",
        "        data_feature = (self.data_feature[index])\n",
        "        data_target = (self.data_target[index])\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(data_feature)\n",
        "\n",
        "        return sample,data_target\n",
        "\n",
        "\n",
        "        return data_feature,data_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHuACwa6p7FD"
      },
      "outputs": [],
      "source": [
        "\n",
        "#parameter of the DNN\n",
        "D_in=784#(28X28 images)\n",
        "H1=200 # number of neurons first hidden layer\n",
        "H2=200 # number of neurons second hidden layer\n",
        "H3=200 # number of neurons third hidden layer\n",
        "nb_class=10 # number of classes\n",
        "nb_class=10\n",
        "nb_models=20\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DPV-vH0KPkB"
      },
      "source": [
        "# MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qauEbyq9n9j1"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainset = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transform)\n",
        "\n",
        "trainloader_mnist = torch.utils.data.DataLoader(trainset, batch_size=N, shuffle=True, num_workers=0)\n",
        "testloader_mnist = torch.utAils.data.DataLoader(testset, batch_size=N, shuffle=False, num_workers=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysDSIVwjKU-C"
      },
      "source": [
        "# Loading NotMNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHNML_Cxn6Ma"
      },
      "outputs": [],
      "source": [
        "\n",
        "folders_notMNIST = os.listdir('/content/drive/MyDrive/BAYESIAN_DNN/base/notMNIST')\n",
        "NotMNIST_x_list, NotMNIST_y_list_onehot, NotMNIST_y_list = [], [], []\n",
        "\n",
        "for idx, folder in enumerate(folders_notMNIST):\n",
        "    files_notMNIST = os.listdir(f'/content/drive/MyDrive/Bayesian_1/notMNIST/{folder}')\n",
        "    for file in files_notMNIST:\n",
        "        img_NotMNIST = cv2.imread(f'/content/drive/MyDrive/Bayesian_1/notMNIST/{folder}/{file}', 0)\n",
        "        NotMNIST_x_list.append(img_NotMNIST)\n",
        "        label_temp = np.zeros([10])\n",
        "        label_temp[idx] = 1\n",
        "        NotMNIST_y_list_onehot.append(label_temp)\n",
        "        NotMNIST_y_list.append(idx)\n",
        "\n",
        "NotMNIST_x = np.stack(NotMNIST_x_list, axis=0)\n",
        "NotMNIST_y = np.asarray(np.stack(NotMNIST_y_list, axis=0), np.int64)\n",
        "print(f\"NotMNIST X shape: {NotMNIST_x.shape}\")\n",
        "print(f\"NotMNIST Y shape: {NotMNIST_y.shape}\")\n",
        "\n",
        "trainloader_NOTmnist = torch.utils.data.DataLoader(\n",
        "    DatasetFromFolder(NotMNIST_x, NotMNIST_y, transform=transform, phase='test'),\n",
        "    batch_size=N, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFRzaFg0p7FE"
      },
      "outputs": [],
      "source": [
        "#collects the name of all Batch Normalization\n",
        "def list_bn_fc(model):\n",
        "    list_bn=[]\n",
        "    for name, module1 in model.named_modules():\n",
        "\n",
        "         if  isinstance(module1, torch.nn.modules.batchnorm._BatchNorm):\n",
        "             list_bn.append(name)\n",
        "\n",
        "    return list_bn\n",
        "\n",
        "# Transform the parameter in model_kalman as per model\n",
        "def prepare_mu_model(model, model_kalman):\n",
        "    # Get the state_dict from the Kalman model and list of batch normalization layers\n",
        "    state_dict = model_kalman.state_dict()\n",
        "    list_bn = list_bn_fc(model)\n",
        "\n",
        "    # Update the state_dict based on whether the parameter belongs to a BatchNorm layer or not\n",
        "    for name, param in state_dict.items():\n",
        "        if any(name_bn in name for name_bn in list_bn):\n",
        "            transformed_param = param\n",
        "        else:\n",
        "            transformed_param = torch.zeros_like(param)\n",
        "\n",
        "\n",
        "        state_dict[name] = transformed_param\n",
        "\n",
        "    # Load the updated state_dict into the model_kalman\n",
        "    model_kalman.load_state_dict(state_dict)\n",
        "\n",
        "    return model_kalman\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prepare_var_model(model, model_kalman):\n",
        "    # Get the list of BatchNorm layers and the state_dict from the Kalman model\n",
        "    list_bn = list_bn_fc(model)\n",
        "    state_dict = model_kalman.state_dict()\n",
        "\n",
        "    # Update the state_dict based on whether the parameter belongs to a BatchNorm layer or not\n",
        "    for name, param in state_dict.items():\n",
        "        if any(name_bn in name for name_bn in list_bn):\n",
        "            transformed_param = param\n",
        "        else:\n",
        "            transformed_param = torch.ones_like(param) * np.sqrt(2 / param.size(0))\n",
        "\n",
        "        # Update the parameter in the state_dict\n",
        "        state_dict[name] = transformed_param\n",
        "\n",
        "\n",
        "    model_kalman.load_state_dict(state_dict)\n",
        "\n",
        "    return model_kalman"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zqyDYgbp7FH"
      },
      "outputs": [],
      "source": [
        "#reshaping the input tensor in the forward pass\n",
        "class hiddenlayers2(torch.nn.Module):\n",
        "    def __init__(self, D_in,H1,H2,H3, D_out,p=0.5):\n",
        "\n",
        "        super(hiddenlayers2, self).__init__()\n",
        "        self.D_in=D_in\n",
        "        self.H1 = H1\n",
        "        self.H2 = H2\n",
        "        self.H3 = H3\n",
        "        self.D_out = D_out\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(self.D_in, self.H1)#,bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=H1)\n",
        "        self.fc2 = torch.nn.Linear(self.H1, self.H2)#,bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(num_features=H2)\n",
        "        self.fc3 = torch.nn.Linear(self.H2, self.H3)#,bias=False)\n",
        "        self.bn3 = nn.BatchNorm1d(num_features=H3)\n",
        "        self.fpred = torch.nn.Linear(self.H3, self.D_out)#,bias=False)\n",
        "        self.p = p\n",
        "        torch.nn.init.kaiming_normal_(self.fc1.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "        torch.nn.init.kaiming_normal_(self.fc2.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "        torch.nn.init.kaiming_normal_(self.fc3.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "        torch.nn.init.kaiming_normal_(self.fpred.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "        nn.init.constant_(self.fc1.bias,0)\n",
        "        nn.init.constant_(self.fc2.bias, 0)\n",
        "        nn.init.constant_(self.fc3.bias, 0)\n",
        "        nn.init.constant_(self.fpred.bias, 0)\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(x.shape[0], -1) # This reshape is needed for the BN update\n",
        "        h1 = F.relu(self.bn1(self.fc1(x)))\n",
        "        h_relu = F.relu(self.bn2(self.fc2(h1)))\n",
        "        h_relu = F.dropout(h_relu, p=self.p, training=self.training)\n",
        "        h_relu = F.relu(self.bn3(self.fc3(h_relu)))\n",
        "        h_relu = F.dropout(h_relu, p=self.p, training=self.training)\n",
        "        y_pred = self.fpred(h_relu)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "class hiddenlayers(torch.nn.Module):\n",
        "    def __init__(self, D_in,H1,H2,H3, D_out,p=0.5):\n",
        "\n",
        "        super(hiddenlayers, self).__init__()\n",
        "        self.D_in=D_in\n",
        "        self.H1 = H1\n",
        "        self.H2 = H2\n",
        "        self.H3 = H3\n",
        "        self.D_out = D_out\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(self.D_in, self.H1)#,bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=H1)\n",
        "        self.fc2 = torch.nn.Linear(self.H1, self.H2)#,bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(num_features=H2)\n",
        "        self.fc3 = torch.nn.Linear(self.H2, self.H3)#,bias=False)\n",
        "        self.bn3 = nn.BatchNorm1d(num_features=H3)\n",
        "        self.fpred = torch.nn.Linear(self.H3, self.D_out)#,bias=False)\n",
        "        self.p = p\n",
        "        torch.nn.init.kaiming_normal_(self.fc1.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "        torch.nn.init.kaiming_normal_(self.fc2.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "        torch.nn.init.kaiming_normal_(self.fc3.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "        torch.nn.init.kaiming_normal_(self.fpred.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "        nn.init.constant_(self.fc1.bias,0)\n",
        "        nn.init.constant_(self.fc2.bias, 0)\n",
        "        nn.init.constant_(self.fc3.bias, 0)\n",
        "        nn.init.constant_(self.fpred.bias, 0)\n",
        "    def forward(self, x):\n",
        "\n",
        "        h1 = F.relu(self.bn1(self.fc1(x)))\n",
        "        h_relu = F.relu(self.bn2(self.fc2(h1)))\n",
        "        h_relu = F.dropout(h_relu, p=self.p, training=self.training)\n",
        "        h_relu = F.relu(self.bn3(self.fc3(h_relu)))\n",
        "        h_relu = F.dropout(h_relu, p=self.p, training=self.training)\n",
        "        y_pred = self.fpred(h_relu)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D8teQ9pKdTF"
      },
      "source": [
        "# MODEL INITIALISATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlAPpskSp7FF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Network Parameters\n",
        "D_in = 784  # Input size (28x28 images)\n",
        "H1 = H2 = H3 = 200  # Hidden layers\n",
        "nb_class = 10  # Number of classes\n",
        "\n",
        "# Initialize models\n",
        "net = hiddenlayers(D_in, H1, H2, H3, nb_class, 0.1)\n",
        "net_mu = hiddenlayers(D_in, H1, H2, H3, nb_class, 0.1)\n",
        "net_var = hiddenlayers(D_in, H1, H2, H3, nb_class, 0.1)\n",
        "\n",
        "net_mu = prepare_mu_model(net, net_mu)\n",
        "net_var = prepare_var_model(net, net_var)\n",
        "print(f\"net_mu and net_variance initialized.\")\n",
        "\n",
        "# Move models to device\n",
        "net, net_mu, net_var = net.to(device), net_mu.to(device), net_var.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=reg_w_decay)\n",
        "lr_scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EinIv9fFp7FF"
      },
      "outputs": [],
      "source": [
        "#Functions\n",
        "\n",
        "#get the current learning rate\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "#update the parameters of models using a Kalman filter\n",
        "def track_mean_var(net1, net2,net3,optim,n_iter,lendata, PK_mu=0, PK_var=0,epoch_limit=10):\n",
        "\n",
        "    list_bn = list_bn_fc(net1)\n",
        "    lr0 = get_lr(optim)\n",
        "    Q_fc1k_minus_1 = 1e-4 #process noise estimate\n",
        "    Q_fc1k_minus_1_var = 5e-5\n",
        "\n",
        "    Pk_fc1_minus = PK_mu + Q_fc1k_minus_1 #prior estimate for mean\n",
        "    K_kfc1 = Pk_fc1_minus / (Pk_fc1_minus + 1e1 )# kalman gain at step k\n",
        "    Pk_fc1_minus_var= PK_var + Q_fc1k_minus_1_var #prior estimate for variance\n",
        "    K_kfc1_var = Pk_fc1_minus_var / (Pk_fc1_minus_var + 1e1)\n",
        "\n",
        "\n",
        "    state_dict_mu_kalman = net2.state_dict()\n",
        "    state_dict_var_kalman = net3.state_dict()\n",
        "\n",
        "    for name, param in net1.named_parameters():\n",
        "\n",
        "\n",
        "        isbn = False\n",
        "        for name_bn in list_bn:\n",
        "            if name_bn in name:\n",
        "                isbn = True\n",
        "        if isbn:\n",
        "\n",
        "            transformed_param=param\n",
        "            transformed_param_var = param\n",
        "\n",
        "        else:\n",
        "\n",
        "            transformed_param = (1 - K_kfc1) * (\n",
        "                    (net2.state_dict()[name]) - lr0 * param.grad) + K_kfc1 * param\n",
        "\n",
        "            transformed_param_var = (1 - K_kfc1_var) * ((net3.state_dict()[name]) + (lr0 * param.grad) * (\n",
        "                    lr0 * param.grad)) + K_kfc1_var * (param - transformed_param) * (param - transformed_param)\n",
        "\n",
        "\n",
        "        state_dict_mu_kalman[name].copy_(transformed_param)\n",
        "\n",
        "        state_dict_var_kalman[name].copy_(transformed_param_var)\n",
        "        PK_mu = (1 - K_kfc1) * Pk_fc1_minus\n",
        "        PK_var = (1 - K_kfc1_var) * Pk_fc1_minus_var\n",
        "    net2.load_state_dict(state_dict_mu_kalman)\n",
        "    net3.load_state_dict(state_dict_var_kalman)\n",
        "    if n_iter%lendata==1 : print(K_kfc1_var)\n",
        "\n",
        "    return PK_mu,PK_var\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NNoDAGEp7FG"
      },
      "outputs": [],
      "source": [
        "#TRAINING AND TESTING FUNCTIONS\n",
        "\n",
        "def train(model, model_mu, model_var, device, train_loader, optimizer, epoch, Pk_fc_mu, Pk_fc_var, log_interval=50):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        inputs, target = data\n",
        "        inputs, target = inputs.to(device), target.to(device)\n",
        "        inputs = inputs.view(inputs.shape[0], -1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        n_iter = epoch * len(train_loader) + batch_idx\n",
        "        with torch.no_grad():\n",
        "            Pk_fc_mu, Pk_fc_var = track_mean_var(\n",
        "                model, model_mu, model_var, optimizer, n_iter, len(train_loader), Pk_fc_mu, Pk_fc_var)\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(f\"Train Epoch: {epoch}, [{batch_idx * len(inputs)}/{len(train_loader.dataset)}]\\tLoss: {loss.item():.6f}\")\n",
        "\n",
        "    lr_scheduler.step()\n",
        "    return Pk_fc_mu, Pk_fc_var\n",
        "losses=[]\n",
        "accuracies=[]\n",
        "def test( model, device,optimizer, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(test_loader, 0):\n",
        "            inputs, target = data\n",
        "            inputs, target = inputs.to(device), target.to(device)\n",
        "            inputs = inputs.view(inputs.shape[0], -1)\n",
        "            output = model(inputs)\n",
        "            test_loss += criterion(output, target)# F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print('CHECK', len(test_loader))\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    losses.append(test_loss)\n",
        "    accuracies.append(accuracy)\n",
        "    return correct\n",
        "\n",
        "def plot_loss_accuracy():\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(losses, accuracies, marker='o', label='Accuracy vs Loss')\n",
        "    plt.xlabel('Average Loss')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Accuracy vs. Average Loss')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def test_uncertainty( model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    BS=0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(test_loader, 0):\n",
        "\n",
        "            inputs, target = data\n",
        "            inputs, target = inputs.to(device), target.to(device)\n",
        "            #inputs = inputs.view(inputs.shape[0], -1)\n",
        "            output = model(inputs)\n",
        "            if batch_idx ==0:\n",
        "                output_concat = output.clone()\n",
        "                target_concat = target.clone()\n",
        "            #print('output_proba',output_proba)\n",
        "            else:\n",
        "                output_concat=torch.cat((output_concat, output), 0)\n",
        "                target_concat=torch.cat((target_concat, target), 0)\n",
        "\n",
        "\n",
        "\n",
        "    return output_concat , target_concat\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NSSeyg4p7FG"
      },
      "outputs": [],
      "source": [
        "#RETRAINING\n",
        "\n",
        "\n",
        "def save_checkpoint_kalman(model, model_mu, model_var, optimizer, epoch, miou, args=None,args_dico=None):\n",
        "\n",
        "    if args!=None:\n",
        "        name = args.name\n",
        "        save_dir = args.save_dir\n",
        "    else:\n",
        "        name = args_dico['name']\n",
        "        save_dir = args_dico['save_dir']\n",
        "\n",
        "    assert os.path.isdir(\n",
        "        save_dir), \"The directory \\\"{0}\\\" doesn't exist.\".format(save_dir)\n",
        "\n",
        "    # Save model\n",
        "    model_path = os.path.join(save_dir, name)\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'miou': miou,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'net_mu': model_mu,\n",
        "        'net_var': model_var,\n",
        "    }\n",
        "    torch.save(checkpoint, model_path)\n",
        "    if args != None:\n",
        "        # Save arguments\n",
        "        summary_filename = os.path.join(save_dir, name + '_summary.txt')\n",
        "        with open(summary_filename, 'w') as summary_file:\n",
        "            sorted_args = sorted(vars(args))\n",
        "            summary_file.write(\"ARGUMENTS\\n\")\n",
        "            for arg in sorted_args:\n",
        "                arg_str = \"{0}: {1}\\n\".format(arg, getattr(args, arg))\n",
        "                summary_file.write(arg_str)\n",
        "\n",
        "            summary_file.write(\"\\nBEST VALIDATION\\n\")\n",
        "            summary_file.write(\"Epoch: {0}\\n\". format(epoch))\n",
        "            summary_file.write(\"Mean IoU: {0}\\n\". format(miou))\n",
        "\n",
        "\n",
        "Pk_fc_mu, Pk_fc_var = 0, 0\n",
        "best_correct = 0\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    Pk_fc_mu, Pk_fc_var = train(net, net_mu, net_var, device, trainloader_mnist, optimizer, epoch, Pk_fc_mu, Pk_fc_var)\n",
        "    correct = test(net, device, optimizer, testloader_mnist)\n",
        "    if correct > best_correct:\n",
        "        save_checkpoint_kalman(net, net_mu, net_var, optimizer, epoch, 0, args_dico=args_dico)\n",
        "        best_correct = correct\n",
        "plot_loss_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As9Uuhhvp7FH"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint_kalman(model,model_kalman_mu,model_kalman_var, optimizer, folder_dir, filename):\n",
        "\n",
        "    assert os.path.isdir(\n",
        "        folder_dir), \"The directory \\\"{0}\\\" doesn't exist.\".format(folder_dir)\n",
        "\n",
        "    # Create folder to save model and information\n",
        "    model_path = os.path.join(folder_dir, filename)\n",
        "    assert os.path.isfile(\n",
        "        model_path), \"The model file \\\"{0}\\\" doesn't exist.\".format(filename)\n",
        "\n",
        "    # Load the stored model parameters to the model instance\n",
        "    checkpoint = torch.load(model_path)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    miou = checkpoint['miou']\n",
        "    model_kalman_mu.load_state_dict(checkpoint['net_mu'].state_dict())\n",
        "    model_kalman_var.load_state_dict(checkpoint['net_var'].state_dict())\n",
        "\n",
        "\n",
        "\n",
        "    return model,model_kalman_mu,model_kalman_var\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYVW4ekgp7FH"
      },
      "outputs": [],
      "source": [
        "#LOAD CHECKPOINTS\n",
        "\n",
        "# Load the Kalman checkpoint\n",
        "\n",
        "net, net_mu, net_var = load_checkpoint_kalman(\n",
        "    net, net_mu, net_var, optimizer, args_dico['save_dir'], args_dico['name']\n",
        ")\n",
        "\n",
        "print('The training is finished!')\n",
        "print('------------------------------------DONE------------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLRgtpBzp7FH"
      },
      "outputs": [],
      "source": [
        "def load_fullnet_kalman(net1, net_mu,net_var,net4, sigma=1, dimfeature=10):\n",
        "\n",
        "    state_dict = net4.state_dict()\n",
        "    list_bn=list_bn_fc(net1)\n",
        "    net4.load_state_dict(net1.state_dict(), strict=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for name, param in net1.named_parameters():\n",
        "            # Transform the parameter as required.\n",
        "            isbn = False\n",
        "            for name_bn in list_bn:\n",
        "                if name_bn in name:\n",
        "                    isbn = True\n",
        "            if not isbn:\n",
        "\n",
        "                # print(name)\n",
        "                fc_i_weight_0 = np.random.normal(0, 1, (dimfeature, 1))\n",
        "                var_fc_i_weight = np.sqrt(np.maximum(net_var.state_dict()[name].clone().cpu().data.numpy(), 0))\n",
        "                # var_fc_i_weight = np.sqrt(1e-4)*np.ones_like(model_var.state_dict()[name].clone().cpu().data.numpy())\n",
        "                size_data_fc_i_weight = np.prod(np.shape(var_fc_i_weight))\n",
        "                var_fc_i_weight = np.reshape(var_fc_i_weight, (size_data_fc_i_weight, 1))\n",
        "\n",
        "                Z_approx_i_FC = np.multiply(var_fc_i_weight, np.cos(np.multiply(\n",
        "                    np.reshape(net1.state_dict()[name].clone().cpu().data.numpy(), (size_data_fc_i_weight, 1)),\n",
        "                    np.random.normal(0, sigma, (1, dimfeature))) + np.random.uniform(0, 6.28318, (\n",
        "                    size_data_fc_i_weight, dimfeature))))\n",
        "                # print('name',np.mean(Z_approx_i_FC)*np.sqrt(2 / (dimfeature)))\n",
        "                fc_i_weight_numpy0 = np.reshape(net_mu.state_dict()[name].clone().cpu().data.numpy(),\n",
        "                                                (size_data_fc_i_weight, 1)) + np.sqrt(2 / (dimfeature)) * np.matmul(Z_approx_i_FC, fc_i_weight_0)\n",
        "\n",
        "                fc_i_weight_numpy = fc_i_weight_numpy0.astype(np.float32)\n",
        "                del (fc_i_weight_numpy0)\n",
        "                fc_i_weight_numpy = np.reshape(fc_i_weight_numpy,\n",
        "                                               np.shape(net_mu.state_dict()[name].clone().cpu().data.numpy()))\n",
        "                transformed_param = torch.tensor(fc_i_weight_numpy)\n",
        "                state_dict[name].copy_(transformed_param)\n",
        "\n",
        "\n",
        "\n",
        "        net4.load_state_dict(state_dict)\n",
        "    return net4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8HLEqRwp7FH"
      },
      "outputs": [],
      "source": [
        "def _check_bn_apply(module, flag):\n",
        "    if issubclass(module.__class__, torch.nn.modules.batchnorm._BatchNorm):\n",
        "        flag[0] = True\n",
        "\n",
        "\n",
        "def _check_bn(model):\n",
        "    flag = [False]\n",
        "    model.apply(lambda module: _check_bn_apply(module, flag))\n",
        "    return flag[0]\n",
        "\n",
        "\n",
        "def _reset_bn(module):\n",
        "    if issubclass(module.__class__, torch.nn.modules.batchnorm._BatchNorm):\n",
        "        module.running_mean = torch.zeros_like(module.running_mean)\n",
        "        module.running_var = torch.ones_like(module.running_var)\n",
        "\n",
        "\n",
        "def _get_momenta(module, momenta):\n",
        "    if issubclass(module.__class__, torch.nn.modules.batchnorm._BatchNorm):\n",
        "        momenta[module] = module.momentum\n",
        "\n",
        "\n",
        "def _set_momenta(module, momenta):\n",
        "    if issubclass(module.__class__, torch.nn.modules.batchnorm._BatchNorm):\n",
        "        module.momentum = momenta[module]\n",
        "\n",
        "\n",
        "def bn_update(loader, model, device=None):\n",
        "\n",
        "    if not _check_bn(model):\n",
        "        return\n",
        "    was_training = model.training\n",
        "    model.train()\n",
        "    momenta = {}\n",
        "    model.apply(_reset_bn)\n",
        "    model.apply(lambda module: _get_momenta(module, momenta))\n",
        "    n = 0\n",
        "    for input in loader:\n",
        "        if isinstance(input, (list, tuple)):\n",
        "            input = input[0]\n",
        "        b = input.size(0)\n",
        "\n",
        "        momentum = b / float(n + b)\n",
        "        for module in momenta.keys():\n",
        "            module.momentum = momentum\n",
        "\n",
        "        if device is not None:\n",
        "            input = input.to(device)\n",
        "\n",
        "        model(input)\n",
        "        n += b\n",
        "\n",
        "    model.apply(lambda module: _set_momenta(module, momenta))\n",
        "    model.train(was_training)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgp0d42eKouI"
      },
      "source": [
        "#EVALUATION of NOTMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHEsVs14p7FI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print('EVALUATION NOTMNIST')\n",
        "\n",
        "output = torch.zeros(len(trainloader_NOTmnist.dataset), nb_class).to(device)\n",
        "m = torch.nn.Softmax(dim=1)\n",
        "\n",
        "for step in range(nb_models):\n",
        "    net4 = hiddenlayers2(D_in, H1, H2, H3, nb_class, 0.1).to(device)\n",
        "    net4 = load_fullnet_kalman(net, net_mu, net_var, net4, sigma=1, dimfeature=10)\n",
        "    bn_update(trainloader_mnist, net4, 'cpu')\n",
        "\n",
        "    output_temp, target = test_uncertainty(net4, device, trainloader_NOTmnist)\n",
        "    output += output_temp\n",
        "\n",
        "output = output / nb_models\n",
        "NLL = criterion(output, target).item()\n",
        "output_proba = m(output)\n",
        "\n",
        "pred = output.argmax(dim=1, keepdim=True)\n",
        "target_onehot = torch.nn.functional.one_hot(target, nb_class)\n",
        "\n",
        "BS = ((target_onehot.type(torch.FloatTensor) - output_proba) ** 2).sum().item()\n",
        "correct = pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "scores, _ = output_proba.max(1)\n",
        "scores = scores.view(-1)\n",
        "scores_notmnits = scores.clone()\n",
        "labels0 = target.view(-1)\n",
        "pred0 = pred.view(-1)\n",
        "\n",
        "output_proba_score0 = output_proba.clone().cpu().data.numpy()\n",
        "output_proba_score_NOTMNIST = np.amax(output_proba_score0, axis=1, keepdims=True)\n",
        "\n",
        "# Save Histogram for NOTMNIST\n",
        "plt.hist(output_proba_score_NOTMNIST, bins=np.linspace(0, 1, 11))\n",
        "plt.title(\"Histogram of NOTMNIST (OOD)\")\n",
        "plt.savefig('Histogram_NOT_MNIST_tradi.png')\n",
        "plt.close()\n",
        "\n",
        "print('--------------------------------------DONE------------------------------------')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWf8HYb6KrSe"
      },
      "source": [
        "#EVALUATION: MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3e1JX2op7FJ",
        "outputId": "69b74268-639b-462d-aa6e-dbd4eb73f120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION MNIST (ID)\n",
            "Correct: 0.9816\n",
            "Error: 0.0184\n",
            "BS: 0.0310\n",
            "NLL: 0.0893\n",
            "--------------------------------------DONE------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print('EVALUATION MNIST (ID)')\n",
        "\n",
        "output = torch.zeros(len(testloader_mnist.dataset), nb_class).to(device)\n",
        "m = torch.nn.Softmax(dim=1)\n",
        "\n",
        "for step in range(nb_models):\n",
        "    net4 = hiddenlayers2(D_in, H1, H2, H3, nb_class, 0.1).to(device)\n",
        "    net4 = load_fullnet_kalman(net, net_mu, net_var, net4, sigma=1, dimfeature=10)\n",
        "    bn_update(trainloader_mnist, net4, 'cpu')\n",
        "\n",
        "    output_temp, target = test_uncertainty(net4, device, testloader_mnist)\n",
        "    output += output_temp\n",
        "\n",
        "output = output / nb_models\n",
        "NLL = criterion(output, target).item()\n",
        "output_proba = m(output)\n",
        "\n",
        "pred = output.argmax(dim=1, keepdim=True)\n",
        "target_onehot = torch.nn.functional.one_hot(target, nb_class)\n",
        "\n",
        "BS = ((target_onehot.type(torch.FloatTensor) - output_proba) ** 2).sum().item()\n",
        "correct = pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "scores, _ = output_proba.max(1)\n",
        "scores = scores.view(-1)\n",
        "labels0 = target.view(-1)\n",
        "pred0 = pred.view(-1)\n",
        "\n",
        "# EVALUATION OF THE OOD\n",
        "conf = torch.cat((scores_notmnits, scores), 0)\n",
        "label = torch.cat((10 * torch.ones_like(scores_notmnits.view(-1)).long(), labels0.long()), 0)\n",
        "pred = torch.cat((0 * torch.ones_like(scores_notmnits.view(-1)).long(), pred0.long()), 0)\n",
        "\n",
        "# Save Histogram for MNIST\n",
        "output_proba_score0 = output_proba.clone().cpu().data.numpy()\n",
        "output_proba_score_MNIST = np.amax(output_proba_score0, axis=1, keepdims=True) #maximum confidence score\n",
        "\n",
        "plt.hist(output_proba_score_MNIST, bins=np.linspace(0, 1, 11))\n",
        "plt.title(\"Histogram of MNIST (ID)\")\n",
        "plt.savefig('Histogram_MNIST.png')\n",
        "plt.close()\n",
        "\n",
        "print(f'Correct: {correct / len(testloader_mnist.dataset):.4f}')\n",
        "print(f'Error: {1 - correct / len(testloader_mnist.dataset):.4f}')\n",
        "print(f'BS: {BS / len(testloader_mnist.dataset):.4f}')\n",
        "print(f'NLL: {NLL:.4f}')\n",
        "print('--------------------------------------DONE------------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sSuThumONeNp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}